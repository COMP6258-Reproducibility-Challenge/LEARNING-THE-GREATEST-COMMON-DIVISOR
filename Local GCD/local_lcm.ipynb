{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c5147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "\n",
    "class MathTokenizer:\n",
    "    def __init__(self, base: int = 10):\n",
    "        self.base = base\n",
    "        self.special_tokens = ['[PAD]', '[SOS]', '[EOS]', '[UNK]']\n",
    "        self.pad_token, self.sos_token, self.eos_token, self.unk_token = self.special_tokens\n",
    "\n",
    "        \n",
    "        self.digits = [str(i) for i in range(base)]\n",
    "\n",
    "        # Vocabulary: special tokens + signs + digit symbols\n",
    "        self.vocab = self.special_tokens + ['+', '-'] + self.digits\n",
    "        self.token2id = {tok: idx for idx, tok in enumerate(self.vocab)}\n",
    "        self.id2token = {idx: tok for tok, idx in self.token2id.items()}\n",
    "\n",
    "    def _int_to_base(self, n: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert a non-negative integer to its digit list in the current base.\n",
    "        Returns a list of digit symbols (strings).\n",
    "        \"\"\"\n",
    "        if n == 0:\n",
    "            return [self.digits[0]]\n",
    "        digits: List[str] = []\n",
    "        while n > 0:\n",
    "            digits.append(self.digits[n % self.base])\n",
    "            n //= self.base\n",
    "        return list(reversed(digits))\n",
    "\n",
    "    def encode(self, sequence: Union[str, List[str]]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode a sequence (either a string of single-character tokens or a list of token strings)\n",
    "        into token IDs, adding SOS and EOS.\n",
    "        \"\"\"\n",
    "        if isinstance(sequence, str):\n",
    "            # legacy: split string into single-character tokens\n",
    "            tokens = [self.sos_token] + list(sequence) + [self.eos_token]\n",
    "        else:\n",
    "            # sequence is already a list of token strings\n",
    "            tokens = [self.sos_token] + sequence + [self.eos_token]\n",
    "        return [self.token2id.get(tok, self.token2id[self.unk_token]) for tok in tokens]\n",
    "\n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs to the sequence of token strings,\n",
    "        stripping out special tokens.\n",
    "        \"\"\"\n",
    "        tokens = [self.id2token.get(i, self.unk_token) for i in ids]\n",
    "        # Remove special tokens\n",
    "        return [tok for tok in tokens if tok not in (self.sos_token, self.eos_token, self.pad_token)]\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        position_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_model) for j in range(d_model)]\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "        position_enc = torch.Tensor(position_enc)\n",
    "        pe = torch.zeros_like(position_enc)\n",
    "        pe[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "        pe[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n",
    "\n",
    "class LCMTransformer(nn.Module):\n",
    "    def __init__(self, tokenizer, d_model=128, nhead=8, num_layers=3, max_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.vocab)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, self.vocab_size)\n",
    "        self.pad_id = tokenizer.token2id[tokenizer.pad_token]\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # src: (batch_size, src_len)\n",
    "        # tgt: (batch_size, tgt_len)\n",
    "        \n",
    "        # Embedding + positional encoding\n",
    "        src_emb = self.pos_encoder(self.embedding(src))\n",
    "        tgt_emb = self.pos_encoder(self.embedding(tgt))\n",
    "        \n",
    "        # Create masks\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "        \n",
    "        # Key padding masks must be 2D (batch_size, seq_len)\n",
    "        src_key_padding_mask = (src == self.pad_id)\n",
    "        tgt_key_padding_mask = (tgt == self.pad_id)\n",
    "        \n",
    "        # Transformer forward\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.fc_out(output)\n",
    "\n",
    "    def predict(self, a: int, b: int, max_length: int = 20) -> int | None:\n",
    "        \"\"\"Predict lcm for integers a and b, in whatever base this tokenizer uses.\"\"\"\n",
    "        # 1) Build the input token list properly, not as one big string\n",
    "        digits_a = self.tokenizer._int_to_base(abs(a))\n",
    "        digits_b = self.tokenizer._int_to_base(abs(b))\n",
    "        src_tokens = ['+'] + digits_a + ['+'] + digits_b\n",
    "\n",
    "        # 2) Encode & send through the model\n",
    "        src_ids = self.tokenizer.encode(src_tokens)\n",
    "        src = torch.tensor(src_ids, device=next(self.parameters()).device).unsqueeze(0)\n",
    "\n",
    "        sos_id = self.tokenizer.token2id[self.tokenizer.sos_token]\n",
    "        eos_id = self.tokenizer.token2id[self.tokenizer.eos_token]\n",
    "\n",
    "        tgt_ids = [sos_id]\n",
    "        for _ in range(max_length):\n",
    "            tgt = torch.tensor(tgt_ids, device=src.device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = self(src, tgt)\n",
    "            next_id = logits.argmax(-1)[0, -1].item()\n",
    "            tgt_ids.append(next_id)\n",
    "            if next_id == eos_id:\n",
    "                break\n",
    "\n",
    "        # 3) Decode the token IDs back to digit‐strings\n",
    "        pred_tokens = self.tokenizer.decode(tgt_ids[1:])  # skip the SOS\n",
    "\n",
    "        # 4) Convert the list of digit‐strings into an integer\n",
    "        try:\n",
    "            value = 0\n",
    "            for tok in pred_tokens:\n",
    "                # skip any stray '+' signs\n",
    "                if tok == '+':\n",
    "                    continue\n",
    "                digit = self.tokenizer.digits.index(tok)\n",
    "                value = value * self.tokenizer.base + digit\n",
    "            return value\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "class LCMDataset(Dataset):\n",
    "    def __init__(self, max_num=100, num_samples=100000, seed=42, test=False, base=10):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.sample_generated = {i:0 for i in range (1,max_num+1)}\n",
    "        self.tokenizer = MathTokenizer(base)\n",
    "        self.data = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            if test:\n",
    "                lcm = self.rng.randint(1, 100000)\n",
    "                divisors = [d for d in range(1,lcm+1) if lcm % d == 0]\n",
    "                while True:\n",
    "                    a, b = np.random.choice(divisors, 2)\n",
    "                    if math.lcm(a,b) == lcm:\n",
    "                        break\n",
    "            else:        \n",
    "                a, b = self.rng.randint(1, np.sqrt(max_num), size=2)\n",
    "                lcm = math.lcm(a,b)\n",
    "                \n",
    "                \n",
    "            self.sample_generated[lcm] += 1\n",
    "            sign_a = ['+']\n",
    "            sign_b = ['+']\n",
    "            \n",
    "            a = self.tokenizer._int_to_base(abs(a))\n",
    "            b = self.tokenizer._int_to_base(abs(b))\n",
    "            lcm = self.tokenizer._int_to_base(abs(lcm))\n",
    "            \n",
    "            src = sign_a + a + sign_b + b\n",
    "            tgt = [\"+\"] + lcm\n",
    "            \n",
    "            self.data.append((\n",
    "                self.tokenizer.encode(src),\n",
    "                self.tokenizer.encode(tgt)\n",
    "            ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.data[idx][0]),\n",
    "            torch.tensor(self.data[idx][1])\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src = nn.utils.rnn.pad_sequence(src_batch, padding_value=tokenizer.token2id[tokenizer.pad_token], batch_first=True)\n",
    "    tgt = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=tokenizer.token2id[tokenizer.pad_token], batch_first=True)\n",
    "    return src, tgt\n",
    "\n",
    "def base_to_int(s, b):\n",
    "    num = 0\n",
    "    for digit in s:\n",
    "        num = num * b + int(digit)\n",
    "    return num\n",
    "\n",
    "def compute_accuracy(model, dataloader, device, max_int, base=10):\n",
    "    model.eval()\n",
    "    perfect_sequences = 0\n",
    "    total_sequences = 0\n",
    "    lcm_correct = defaultdict(int)\n",
    "    lcm_total = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            # Get model predictions\n",
    "            output = model(src, tgt_input)\n",
    "            preds = output.argmax(-1)\n",
    "                  \n",
    "            # 1. Move all sequences to CPU at once\n",
    "            tgt_np = tgt_output.cpu().numpy()\n",
    "            preds_np = preds.cpu().numpy()\n",
    "            pad_id = model.pad_id\n",
    "            \n",
    "            # 2. Process all sequences in batch\n",
    "            for seq_idx in range(tgt_output.shape[0]):\n",
    "                # Get non-pad tokens\n",
    "                target_tokens = tgt_np[seq_idx][1:]\n",
    "                pred_tokens = preds_np[seq_idx][1:]\n",
    "                \n",
    "                # Skip empty sequences\n",
    "                if len(target_tokens) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Batch decode using tokenizer (more efficient than one-by-one)\n",
    "                try:\n",
    "                    correct_lcm = tokenizer.decode(target_tokens)\n",
    "                    predicted_lcm = tokenizer.decode(pred_tokens)\n",
    "                    \n",
    "                    if base!=10:\n",
    "                        correct_lcm = base_to_int(correct_lcm, base)\n",
    "                        predicted_lcm = base_to_int(predicted_lcm, base)\n",
    "                            \n",
    "                    lcm_total[correct_lcm] += 1\n",
    "                    if predicted_lcm == correct_lcm:\n",
    "                        lcm_correct[correct_lcm] += 1\n",
    "                except (ValueError, AttributeError):\n",
    "                    continue\n",
    "    \n",
    "    # Sort LCM values in ascending order\n",
    "    sorted_lcms = sorted(lcm_total.keys())\n",
    "    sorted_per_lcm = OrderedDict((k, lcm_correct[k]/lcm_total[k]) for k in sorted_lcms)\n",
    "    sorted_correct = OrderedDict((k, lcm_correct[k]) for k in sorted_lcms)\n",
    "    sorted_total = OrderedDict((k, lcm_total[k]) for k in sorted_lcms)\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': sum(lcm_correct.values()) / max(sum(lcm_total.values()), 1),\n",
    "        'per_lcm_accuracy': sorted_per_lcm,\n",
    "        'correct_counts': sorted_correct,\n",
    "        'total_counts': sorted_total,\n",
    "        'sorted_lcms': sorted_lcms  # List of LCM values in order\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5fc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xw3g19\\AppData\\Local\\Temp\\ipykernel_26528\\4181782398.py:70: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  pe[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
      "C:\\Users\\xw3g19\\AppData\\Local\\Temp\\ipykernel_26528\\4181782398.py:71: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  pe[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.185579536823516, Accuracy: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.9656426931949371, Accuracy: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.8331449184011905, Accuracy: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.7238990717745841, Accuracy: 0.0008666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.5867805034556288, Accuracy: 0.0011666666666666668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 1.4931402307875612, Accuracy: 0.0021333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 1.440169089905759, Accuracy: 0.0025666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 1.407294309900162, Accuracy: 0.0028666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 1.3718386944304122, Accuracy: 0.003766666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xw3g19\\.conda\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.3522793384308511, Accuracy: 0.0046\n"
     ]
    }
   ],
   "source": [
    "# Initialize DataFrame to store results\n",
    "per_lcm_df = pd.DataFrame()\n",
    "\n",
    "# Initialize components\n",
    "validate_step = 1\n",
    "layers = 2\n",
    "heads = 4\n",
    "hidden_dimension = 256\n",
    "length = 512\n",
    "lr = 10e-5\n",
    "batch = 128\n",
    "max_int = 1000000\n",
    "sample_size = 10000\n",
    "dropout = 0\n",
    "max_epoch = 10\n",
    "base = 30\n",
    "tokenizer = MathTokenizer(base)\n",
    "seed = None\n",
    "\n",
    "model = LCMTransformer(tokenizer, d_model=hidden_dimension, nhead=heads, num_layers=layers, max_length=length, dropout=dropout)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token2id[tokenizer.pad_token])\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Collect infos on number of lcm generated\n",
    "samples_generated = defaultdict(int)\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_dataset = LCMDataset(max_num=max_int, num_samples=3*sample_size, seed=seed, base=base)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, collate_fn=collate_fn, shuffle=True)\n",
    "    total_sequences = 0\n",
    "    perfect_sequences = 0\n",
    "    \n",
    "    for key in train_dataset.sample_generated:\n",
    "        samples_generated[key] += train_dataset.sample_generated[key] \n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Prepare target input/output\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Get prediction\n",
    "        preds = output.argmax(-1)\n",
    "        \n",
    "        # Mask out padding tokens\n",
    "        mask = (tgt_output != tokenizer.token2id[tokenizer.pad_token])\n",
    "            \n",
    "        loss = criterion(\n",
    "            output.reshape(-1, output.size(-1)),\n",
    "            tgt_output.reshape(-1)\n",
    "        )\n",
    "        \n",
    "        # Compute accuracy\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            mask = (tgt_output != model.pad_id)\n",
    "            seq_match = (preds == tgt_output) | ~mask\n",
    "            perfect_sequences += seq_match.all(dim=1).sum().item()\n",
    "            total_sequences += tgt.size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dataloader)}, Accuracy: {perfect_sequences / max(total_sequences, 1)}\")\n",
    "    if counter % validate_step == 0:\n",
    "        # Accuracy validation\n",
    "        validation_dataset = LCMDataset(max_num=max_int, num_samples=sample_size, seed=seed, test=True, base=base)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=128, collate_fn=collate_fn)\n",
    "        results = compute_accuracy(model, validation_dataloader, \"cuda\", max_int=max_int, base=base)\n",
    "        # Convert per_lcm_accuracy to a DataFrame row\n",
    "        row = pd.DataFrame({\n",
    "            'step': counter,\n",
    "            **results['per_lcm_accuracy']  # Flattens LCMs into columns\n",
    "        }, index=[0])\n",
    "        \n",
    "        # Append to the main DataFrame\n",
    "        per_lcm_df = pd.concat([per_lcm_df, row], ignore_index=True)\n",
    "        \n",
    "        # Optional: Save to CSV periodically\n",
    "        if counter % (validate_step * 10) == 0:  # Save every 10 validations\n",
    "            per_lcm_df.to_csv(\"per_lcm_accuracy_history.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c98fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
